---
title: "Monte Carlo Simulation Tools for REDD+ Uncertainty Estimates"
date: 2024-12-19
author: 
  - name: Seamus Murphy
    orcid: 0000-0002-1792-0351 
    email: seamusrobertmurphy@gmail.com
abstract: > 
  A workflow for deriving [ART Tool](https://verra.org/wp-content/uploads/2024/02/VT0007-Unplanned-Deforestation-Allocation-v1.0.pdf).
keywords:
  - REDD+
  - VCS
  - Verra
  - Carbon verification
  - Jurisdictional
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "**Contents**"
    toc-depth: 5
    toc-expand: 4
    theme: [minimal]
    output-dir: docs
highlight-style: github
df-print: kable
keep-md: true
prefer-html: true
engine: knitr
output-dir: docs
---

```{r setup}
#| warning: false
#| message: false
#| error: false
#| include: false
#| echo: false

#install.packages("easypackages")
easypackages::packages(
  "animation",
  "BIOMASS",
  "dataMaid",
  "dplyr",
  "extrafont",
  "htmltools",
  "janitor",
  "kableExtra",
  "knitr",
  "readxl",
  "tinytex",
  prompt = F)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
  error = TRUE, comment = NA, tidy.opts = list(width.cutoff = 60)) 
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
sf::sf_use_s2(use_s2 = FALSE)
```

# Introduction

#### Scope of Work

## Import data {#sec-1.1}

```{r dummy-import}
#| warning: false
#| message: false
#| error: false
#| echo: true
set.seed(333)
dataset_tidy = read.csv("./data/dataset_tidy.csv")
write.csv(dataset_tidy, "./data/dataset_tidy.csv", row.names = FALSE)
dataset_tidy |> kbl(caption = "Table 1: Dummy dataset derived for pilot test") |> kable_styling()
```

## Compute biomass

$$V_{j,i|sp} = \sum_{l = 1}^{L} V_{l,j,i,sp}$$

|          |                            |
|:--------:|----------------------------|
|  $$V$$   | sum of merchantable volume ($m^3$) |
|  $$j$$   | of species                 |
|  $$sp$$  | plot                       |
|  $$i$$   | stratum                    |

```{r eq1}
#| warning: false
#| message: false
#| error: false
#| eval: true
#| echo: true

dataset_tidy = dataset_tidy |>
  group_by(species_j, stratum_i, plot_sp) |>
  mutate(vji_sp_m3 = sum(volume))

# compute new variable 'vji_sp_m3'
data.table::setDT(dataset_tidy)[, .(
  vji_sp_m3 = sum(volume)
  ),
  by = .(stratum_i, plot_sp, species_j)
] |> kbl() |> kable_styling()
```

## Variable descriptives



`control <- trainControl(method = "LGOCV", number = 10, p=.9)`

you will perform 10 repetitions of leave group validation where for each repetition 90% (p = 0.9) of the data will be sampled at random and used for training while the remaining 10% of data will be used for testing. This is also called Monte-Carlo Cross Validation. Usually more repetitions are performed with MC-CV.

When using

`control <- trainControl(method = "cv", number = 10)`

the data set will be split into 10 parts and 10 resampling iterations will be performed. In each iteration 9 parts will be used for training and the remaining part will be used for testing. This will progress until all the parts are used once for testing. This is called K-fold cross validation. In this case K is 10.

In each case around 90% of the data will be used for training in each resampling iteration.

**EDIT**: I have changed the above code

`control <- trainControl(method = "cv", number = 10, p=.9)`

to

`control <- trainControl(method = "LGOCV", number = 10, p=.9)`

since if you specify `cv` you will actually preform k-fold CV and the p argument will be ignored. So in order to perform leave group out cross validation you must specify `method = "LGOCV"` and then the `p` argument will be used to determine the train/test split ratio.

Simulation regime

see also LGOCV: `load(url("http://caret.r-forge.r-project.org/exampleModels.RData"))`

```{r}
#| eval: false

model_training_time_series <- trainControl(
  method = "timeslice",
  initialWindow = 36,
  horizon = 12,
  fixedWindow = TRUE
)

model_training_10kfold <- trainControl(
  method = "repeatedcv",
  number = 10, repeats = 10
)
# animation of 10-kfold method:
knitr::include_graphics(path = "animation.gif")
```

## Training-Test Split

```{r}
#| eval: false

samples <- createDataPartition(dataset$variable_indexed, p = 0.70, list = FALSE)
train.data <- dataset[samples, ]
test.data <- dataset[-samples, ]


model_training_time_series <- trainControl(
  method = "timeslice",
  initialWindow = 36,
  horizon = 12,
  fixedWindow = TRUE
)

model_training_10kfold <- trainControl(
  method = "repeatedcv",
  number = 10, repeats = 10
)
# animation of 10-kfold method:
knitr::include_graphics(path = "animation.gif")
```

## Simulation Fitting

```{r}
#| eval: false

# model 1 - NDMI - model specification
svm_ndmi_linear <- train(pi_mpb_killed ~ ndmi,
  data = beetle_train.data,
  method = "svmLinear",
  trControl = model_training_10kfold,
  preProcess = c("center", "scale"),
  tuneLength = 10
)
```

## Simulation Validation

```{r}
#| eval: false

beetle_ndmi_pred_train <- predict(svm_ndmi_linear, data = beetle_train.data)
beetle_ndmi_pred_train_mae <- mae(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_mae

beetle_ndmi_pred_train_mae_rel <- (beetle_ndmi_pred_train_mae / mean(beetle_train.data$pi_mpb_killed)) * 100
beetle_ndmi_pred_train_mae_rel

beetle_ndmi_pred_train_rmse <- rmse(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_rmse

beetle_ndmi_pred_train_rmse_rel <- (beetle_ndmi_pred_train_rmse / mean(beetle_train.data$pi_mpb_killed)) * 100
beetle_ndmi_pred_train_rmse_rel

beetle_ndmi_pred_train_R2 <- R2(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_R2

TheilU(beetle_train.data$pi_mpb_killed, beetle_ndmi_pred_train, type = 2)

beetle_ndmi_pred_train_Ubias <- ((beetle_ndmi_pred_train_mae) * 20) / ((beetle_ndmi_pred_train_mae)^2)
beetle_ndmi_pred_train_Ubias

beetle_ndmi_pred_test <- predict(svm_ndmi_linear, data = darkwoods_beetle_plots_data)
beetle_ndmi_pred_test_rmse <- rmse(beetle_ndmi_pred_test, darkwoods_beetle_plots_data$pi_mpb_killed)
beetle_ndmi_pred_test_rmse / beetle_ndmi_pred_train_rmse
```

Let's assume 𝑁N is the size of the dataset, 𝑘k is the number of the 𝑘k-fold subsets , 𝑛𝑡nt is the size of the training set and 𝑛𝑣nv is the size of the validation set. Therefore, 𝑁=𝑘×𝑛𝑣N=k×nv for 𝑘k-fold cross-validation and 𝑁=𝑛𝑡+𝑛𝑣N=nt+nv for Monte Carlo cross-validation.

𝑘k-fold cross-validation (kFCV) divides the 𝑁N data points into 𝑘k mutually exclusive subsets of equal size. The process then leaves out one of the 𝑘k subsets as a validation set and trains on the remaining subsets. This process is repeated 𝑘k times, leaving out one of the 𝑘k subsets each time. The size of 𝑘k can range from 𝑁N to 22 (𝑘=𝑁k=N is called leave-one-out cross validation). The authors in \[2\] suggest setting 𝑘=5k=5 or 1010.

**Monte Carlo cross-validation** (MCCV) simply splits the 𝑁N data points into the two subsets 𝑛𝑡nt and 𝑛𝑣nv by sampling, without replacement, 𝑛𝑡nt data points. The model is then trained on subset 𝑛𝑡nt and validated on subset 𝑛𝑣nv.There exist (𝑁𝑛𝑡)(Nnt) unique training sets, but MCCV avoids the need to run this many iterations. Zhang \[3\] shows that running MCCV for 𝑁2N2 iterations has results close to cross validation over all (𝑁𝑛𝑡)(Nnt) unique training sets. It should be noted that the literature lacks research for large N.

The choice of 𝑘k and 𝑛𝑡nt affects the bias/variance trade off. The larger 𝑘k or 𝑛𝑡nt, the lower the bias and the higher the variance. Larger training sets are more similar between iterations, hence over fitting to the training data. See \[2\] for more on this discussion. The bias and variance of kFCV and MCCV are different, but the bias of the two methods can be made equal by choosing appropriate levels of 𝑘k and 𝑛𝑡nt. The values of the bias and variance for both methods are shown in \[1\] (this paper refers to MCCV as repeated-learning testing-model).

------------------------------------------------------------------------

\[1\] Burman, P. (1989). A Comparative study of ordinary cross-validation, 𝑣v-fold cross validation and the repeated learing testing-model methods. *Bometrika* **76** 503-514.

\[2\] Hastie, T., Tibshirani, R. and Friedman, J. (2011). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second ed.* New York: Springer.

\[3\] Zhang, P. (1993). Model Selection Via Muiltfold Cross Validation. *Ann. Stat.* **21** 299–313
