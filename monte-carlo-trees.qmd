---
title: "Monte Carlo Simulation Tools for REDD+ Uncertainty Estimates"
date: 2024-12-19
author: 
  - name: Seamus Murphy
    orcid: 0000-0002-1792-0351 
    email: seamusrobertmurphy@gmail.com
abstract: > 
  A workflow for deriving [ART Tool](https://verra.org/wp-content/uploads/2024/02/VT0007-Unplanned-Deforestation-Allocation-v1.0.pdf).
keywords:
  - REDD+
  - VCS
  - Verra
  - Carbon verification
  - Jurisdictional
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "**Contents**"
    toc-depth: 5
    toc-expand: 4
    theme: [minimal]
    output-dir: docs
highlight-style: github
df-print: kable
keep-md: true
prefer-html: true
engine: knitr
output-dir: docs
---

```{r setup}
#| warning: false
#| message: false
#| error: false
#| include: false
#| echo: false

#install.packages("easypackages")
easypackages::packages(
  "animation",
  "BIOMASS",
  "dataMaid",
  "dplyr",
  "extrafont",
  "htmltools",
  "janitor",
  "kableExtra",
  "knitr",
  "readxl",
  "tinytex",
  prompt = F)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
  error = TRUE, comment = NA, tidy.opts = list(width.cutoff = 60)) 
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
sf::sf_use_s2(use_s2 = FALSE)
```

# Introduction

#### Scope of Work

## Import data {#sec-1.1}

```{r dummy-import}
#| warning: false
#| message: false
#| error: false
#| echo: true
set.seed(333)
dataset_tidy = read.csv("./data/dataset_tidy.csv")
write.csv(dataset_tidy, "./data/dataset_tidy.csv", row.names = FALSE)
dataset_tidy |> kbl(caption = "Table 1: Dummy dataset derived for pilot test") |> kable_styling()
```

## Compute biomass

$$V_{j,i|sp} = \sum_{l = 1}^{L} V_{l,j,i,sp}$$

|          |                            |
|:--------:|----------------------------|
|  $$V$$   | sum of merchantable volume ($m^3$) |
|  $$j$$   | of species                 |
|  $$sp$$  | plot                       |
|  $$i$$   | stratum                    |

```{r eq1}
#| warning: false
#| message: false
#| error: false
#| eval: true
#| echo: true

dataset_tidy = dataset_tidy |>
  group_by(species_j, stratum_i, plot_sp) |>
  mutate(vji_sp_m3 = sum(volume))

# compute new variable 'vji_sp_m3'
data.table::setDT(dataset_tidy)[, .(
  vji_sp_m3 = sum(volume)
  ),
  by = .(stratum_i, plot_sp, species_j)
] |> kbl() |> kable_styling()
```

## Variable descriptives



`control <- trainControl(method = "LGOCV", number = 10, p=.9)`

you will perform 10 repetitions of leave group validation where for each repetition 90% (p = 0.9) of the data will be sampled at random and used for training while the remaining 10% of data will be used for testing. This is also called Monte-Carlo Cross Validation. Usually more repetitions are performed with MC-CV.

When using

`control <- trainControl(method = "cv", number = 10)`

the data set will be split into 10 parts and 10 resampling iterations will be performed. In each iteration 9 parts will be used for training and the remaining part will be used for testing. This will progress until all the parts are used once for testing. This is called K-fold cross validation. In this case K is 10.

In each case around 90% of the data will be used for training in each resampling iteration.

**EDIT**: I have changed the above code

`control <- trainControl(method = "cv", number = 10, p=.9)`

to

`control <- trainControl(method = "LGOCV", number = 10, p=.9)`

since if you specify `cv` you will actually preform k-fold CV and the p argument will be ignored. So in order to perform leave group out cross validation you must specify `method = "LGOCV"` and then the `p` argument will be used to determine the train/test split ratio.

Simulation regime

see also LGOCV: `load(url("http://caret.r-forge.r-project.org/exampleModels.RData"))`

```{r}
#| eval: false

model_training_time_series <- trainControl(
  method = "timeslice",
  initialWindow = 36,
  horizon = 12,
  fixedWindow = TRUE
)

model_training_10kfold <- trainControl(
  method = "repeatedcv",
  number = 10, repeats = 10
)
# animation of 10-kfold method:
knitr::include_graphics(path = "animation.gif")
```

## Training-Test Split

```{r}
#| eval: false

samples <- createDataPartition(dataset$variable_indexed, p = 0.70, list = FALSE)
train.data <- dataset[samples, ]
test.data <- dataset[-samples, ]


model_training_time_series <- trainControl(
  method = "timeslice",
  initialWindow = 36,
  horizon = 12,
  fixedWindow = TRUE
)

model_training_10kfold <- trainControl(
  method = "repeatedcv",
  number = 10, repeats = 10
)
# animation of 10-kfold method:
knitr::include_graphics(path = "animation.gif")
```

## Simulation Fitting

```{r}
#| eval: false

# model 1 - NDMI - model specification
svm_ndmi_linear <- train(pi_mpb_killed ~ ndmi,
  data = beetle_train.data,
  method = "svmLinear",
  trControl = model_training_10kfold,
  preProcess = c("center", "scale"),
  tuneLength = 10
)
```

## Simulation Validation

```{r}
#| eval: false

beetle_ndmi_pred_train <- predict(svm_ndmi_linear, data = beetle_train.data)
beetle_ndmi_pred_train_mae <- mae(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_mae

beetle_ndmi_pred_train_mae_rel <- (beetle_ndmi_pred_train_mae / mean(beetle_train.data$pi_mpb_killed)) * 100
beetle_ndmi_pred_train_mae_rel

beetle_ndmi_pred_train_rmse <- rmse(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_rmse

beetle_ndmi_pred_train_rmse_rel <- (beetle_ndmi_pred_train_rmse / mean(beetle_train.data$pi_mpb_killed)) * 100
beetle_ndmi_pred_train_rmse_rel

beetle_ndmi_pred_train_R2 <- R2(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_R2

TheilU(beetle_train.data$pi_mpb_killed, beetle_ndmi_pred_train, type = 2)

beetle_ndmi_pred_train_Ubias <- ((beetle_ndmi_pred_train_mae) * 20) / ((beetle_ndmi_pred_train_mae)^2)
beetle_ndmi_pred_train_Ubias

beetle_ndmi_pred_test <- predict(svm_ndmi_linear, data = darkwoods_beetle_plots_data)
beetle_ndmi_pred_test_rmse <- rmse(beetle_ndmi_pred_test, darkwoods_beetle_plots_data$pi_mpb_killed)
beetle_ndmi_pred_test_rmse / beetle_ndmi_pred_train_rmse
```

Let's assume ð‘N is the size of the dataset, ð‘˜k is the number of the ð‘˜k-fold subsets , ð‘›ð‘¡nt is the size of the training set and ð‘›ð‘£nv is the size of the validation set. Therefore, ð‘=ð‘˜Ã—ð‘›ð‘£N=kÃ—nv for ð‘˜k-fold cross-validation and ð‘=ð‘›ð‘¡+ð‘›ð‘£N=nt+nv for Monte Carlo cross-validation.

ð‘˜k-fold cross-validation (kFCV) divides the ð‘N data points into ð‘˜k mutually exclusive subsets of equal size. The process then leaves out one of the ð‘˜k subsets as a validation set and trains on the remaining subsets. This process is repeated ð‘˜k times, leaving out one of the ð‘˜k subsets each time. The size of ð‘˜k can range from ð‘N to 22 (ð‘˜=ð‘k=N is called leave-one-out cross validation). The authors in \[2\] suggest setting ð‘˜=5k=5 or 1010.

**Monte Carlo cross-validation** (MCCV) simply splits the ð‘N data points into the two subsets ð‘›ð‘¡nt and ð‘›ð‘£nv by sampling, without replacement, ð‘›ð‘¡nt data points. The model is then trained on subset ð‘›ð‘¡nt and validated on subset ð‘›ð‘£nv.There exist (ð‘ð‘›ð‘¡)(Nnt) unique training sets, but MCCV avoids the need to run this many iterations. Zhang \[3\] shows that running MCCV for ð‘2N2 iterations has results close to cross validation over all (ð‘ð‘›ð‘¡)(Nnt) unique training sets. It should be noted that the literature lacks research for large N.

The choice of ð‘˜k and ð‘›ð‘¡nt affects the bias/variance trade off. The larger ð‘˜k or ð‘›ð‘¡nt, the lower the bias and the higher the variance. Larger training sets are more similar between iterations, hence over fitting to the training data. See \[2\] for more on this discussion. The bias and variance of kFCV and MCCV are different, but the bias of the two methods can be made equal by choosing appropriate levels of ð‘˜k and ð‘›ð‘¡nt. The values of the bias and variance for both methods are shown in \[1\] (this paper refers to MCCV as repeated-learning testing-model).

------------------------------------------------------------------------

\[1\] Burman, P. (1989). A Comparative study of ordinary cross-validation, ð‘£v-fold cross validation and the repeated learing testing-model methods. *Bometrika* **76** 503-514.

\[2\] Hastie, T., Tibshirani, R. and Friedman, J. (2011). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second ed.* New York: Springer.

\[3\] Zhang, P. (1993). Model Selection Via Muiltfold Cross Validation. *Ann. Stat.* **21** 299â€“313
