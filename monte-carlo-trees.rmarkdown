---
title: "Monte Carlo Simulation Tools for REDD+ Uncertainty Estimates"
date: 2024-12-19
abstract: > 
  A workflow for deriving [ART Tool](https://verra.org/wp-content/uploads/2024/02/VT0007-Unplanned-Deforestation-Allocation-v1.0.pdf).
keywords:
  - REDD+
  - Carbon verification
  - Uncertainty
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "**Contents**"
    toc-depth: 5
    toc-expand: 4
    theme: [minimal]
highlight-style: github
df-print: kable
keep-md: true
prefer-html: true
engine: knitr
output-dir: docs
---

```{r setup}
#| warning: false
#| message: false
#| error: false
#| include: false
#| echo: false

#install.packages("easypackages")
easypackages::packages(
  "allodb",
  "animation",
  "BIOMASS",
  "caret",
  "dataMaid",
  "dplyr",
  "extrafont",
  "FAwR",
  "htmltools",
  "janitor",
  "lattice",
  "kableExtra",
  "knitr",
  "olsrr",
  "readxl",
  "tinytex",
  "psych",
  prompt = F)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
  error = TRUE, comment = NA, tidy.opts = list(width.cutoff = 60)) 
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
sf::sf_use_s2(use_s2 = FALSE)
```



# Introduction



```{r}
#| eval: true

cv.ani(
  x   = runif(150),
  k   = 10,
  col = c("green", "red", "blue"),
  pch = c(4, 1)
)

# animation of 10-kfold method:
knitr::include_graphics(path = "animation.gif")
```




#### Scope of Work

## Import data {#sec-1.1}



```{r dummy-import}
#| warning: false
#| message: false
#| error: false
#| echo: true
set.seed(333)
#data(ufc) # spuRs::vol.m3(dataset$dbh.cm, dataset$height.m, multiplier = 0.5)
data(scbi_stem1)
dataset   = scbi_stem1
dataset |> kbl(
  caption = "Table 1: Dataset from Smithsonian Institute provided by allodb package") |>
  kable_styling() 
str(dataset)
```



## Probability density functions



```{r}
#| layout-ncol: 2

# add allometry database
data(equations)
data("equations_metadata")
show_cols   = c("equation_id", "equation_taxa", "equation_allometry")
eq_tab_acer = new_equations(subset_taxa = "Acer")
head(eq_tab_acer[, show_cols])

# Compute above ground biomass
dataset$agb = allodb::get_biomass(
    dbh     = dataset$dbh,
    genus   = dataset$genus,
    species = dataset$species,
    coords  = c(-78.2, 38.9)
  )

# examine dbh ~ agb function
dbh_agb = lm(dbh ~ agb, data = dataset)
olsrr::ols_test_breusch_pagan(lm(dbh_agb)) #<0.0000
lattice::histogram(dbh ~ agb, data = dataset)
plot(
  x    = dataset$dbh,
  y    = dataset$agb,
  col  = factor(scbi_stem1$genus),
  xlab = "DBH (cm)",
  ylab = "AGB (kg)"
)
abline(dbh_agb, col = "red")

# examine univariate distributions
psych::describe(dataset)
wilcox.test(dataset$dbh) # p<0.00001
wilcox.test(dataset$agb) # p<0.00001
h = hist(dataset$dbh, breaks=10, col="red")
xfit<-seq(min(dataset$dbh),max(dataset$dbh),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(dataset$dbh))
yfit <- yfit*diff(h$mids[1:2])*length(dataset$dbh)
lines(xfit, yfit, col="blue", lwd=2)

h = hist(dataset$agb, breaks=10, col="red")
xfit<-seq(min(dataset$agb),max(dataset$agb),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(dataset$agb))
yfit <- yfit*diff(h$mids[1:2])*length(dataset$agb)
lines(xfit, yfit, col="blue", lwd=2)
```



## Simulation Regime

The `LGOCV` acronym stands for "leave one group out cross validation". We must select the % of test data that is set out from the build upon which the model will be repeatedly trained.



```{r}
# Cross-validation split for bias detection
#samples     = caret::createDataPartition(dataset_tidy$volume, p = 0.80, list = FALSE)
#train_data  = dataset_tidy[samples, ]
#test_data   = dataset_tidy[-samples, ]

# Simulation pattern & regime
monte_carlo = trainControl(
  method    = "LGOCV",
  number    = 100,     # number of simulations
  p         = 0.8)     # percentage resampled


# Training model fit with all covariates (".") & the simulation
lm_monte_carlo = train(
  agb ~ ., 
  data      = dataset, 
  na.action = na.omit,
  trControl = monte_carlo)

lm_monte_carlo 
```



## Plot residuals

To enable access to these predictions, we need to instruct `caret` to retain the resampled predictions by setting `savePredictions = "final"` in our `trainControl()` function. It's important to be aware that if youâ€™re working with a large dataset or numerous resampling iterations, the resulting `train()` object may grow significantly in size. This happens because `caret` must store a record of every row, including both the observed values and predictions, for each resampling iteration. By visualizing the results, we can offer insights into the performance of our model on the resampled data.



```{r}
monte_carlo_viz = trainControl(
  method    = "LGOCV", 
  p         = 0.8,            
  number    = 1,  # just for saving previous results
  savePredictions = "final") 

lm_monte_carlo_viz = train(
  price_log ~ . -price, 
  data      = train_data, 
  method    = "lm",
  na.action = na.omit,
  trControl = monte_carlo_viz)

lm_monte_carlo_viz$pred  # residuals 

lm_monte_carlo_viz$pred |> 
  ggplot(aes(x=pred,y=obs)) +
    geom_point(shape=1) + 
    geom_abline(slope=1, colour='blue')  +
    coord_obs_pred()
```






## Uncertainty Estimates



```{r}
#| eval: false

beetle_ndmi_pred_train <- predict(svm_ndmi_linear, data = beetle_train.data)
beetle_ndmi_pred_train_mae <- mae(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_mae

beetle_ndmi_pred_train_mae_rel <- (beetle_ndmi_pred_train_mae / mean(beetle_train.data$pi_mpb_killed)) * 100
beetle_ndmi_pred_train_mae_rel

beetle_ndmi_pred_train_rmse <- rmse(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_rmse

beetle_ndmi_pred_train_rmse_rel <- (beetle_ndmi_pred_train_rmse / mean(beetle_train.data$pi_mpb_killed)) * 100
beetle_ndmi_pred_train_rmse_rel

beetle_ndmi_pred_train_R2 <- R2(beetle_ndmi_pred_train, beetle_train.data$pi_mpb_killed)
beetle_ndmi_pred_train_R2

TheilU(beetle_train.data$pi_mpb_killed, beetle_ndmi_pred_train, type = 2)

beetle_ndmi_pred_train_Ubias <- ((beetle_ndmi_pred_train_mae) * 20) / ((beetle_ndmi_pred_train_mae)^2)
beetle_ndmi_pred_train_Ubias

beetle_ndmi_pred_test <- predict(svm_ndmi_linear, data = darkwoods_beetle_plots_data)
beetle_ndmi_pred_test_rmse <- rmse(beetle_ndmi_pred_test, darkwoods_beetle_plots_data$pi_mpb_killed)
beetle_ndmi_pred_test_rmse / beetle_ndmi_pred_train_rmse
```



Let's assume ð‘N is the size of the dataset, ð‘˜k is the number of the ð‘˜k-fold subsets , ð‘›ð‘¡nt is the size of the training set and ð‘›ð‘£nv is the size of the validation set. Therefore, ð‘=ð‘˜Ã—ð‘›ð‘£N=kÃ—nv for ð‘˜k-fold cross-validation and ð‘=ð‘›ð‘¡+ð‘›ð‘£N=nt+nv for Monte Carlo cross-validation.

ð‘˜k-fold cross-validation (kFCV) divides the ð‘N data points into ð‘˜k mutually exclusive subsets of equal size. The process then leaves out one of the ð‘˜k subsets as a validation set and trains on the remaining subsets. This process is repeated ð‘˜k times, leaving out one of the ð‘˜k subsets each time. The size of ð‘˜k can range from ð‘N to 22 (ð‘˜=ð‘k=N is called leave-one-out cross validation). The authors in \[2\] suggest setting ð‘˜=5k=5 or 1010.

**Monte Carlo cross-validation** (MCCV) simply splits the ð‘N data points into the two subsets ð‘›ð‘¡nt and ð‘›ð‘£nv by sampling, without replacement, ð‘›ð‘¡nt data points. The model is then trained on subset ð‘›ð‘¡nt and validated on subset ð‘›ð‘£nv.There exist (ð‘ð‘›ð‘¡)(Nnt) unique training sets, but MCCV avoids the need to run this many iterations. Zhang \[3\] shows that running MCCV for ð‘2N2 iterations has results close to cross validation over all (ð‘ð‘›ð‘¡)(Nnt) unique training sets. It should be noted that the literature lacks research for large N.

The choice of ð‘˜k and ð‘›ð‘¡nt affects the bias/variance trade off. The larger ð‘˜k or ð‘›ð‘¡nt, the lower the bias and the higher the variance. Larger training sets are more similar between iterations, hence over fitting to the training data. See \[2\] for more on this discussion. The bias and variance of kFCV and MCCV are different, but the bias of the two methods can be made equal by choosing appropriate levels of ð‘˜k and ð‘›ð‘¡nt. The values of the bias and variance for both methods are shown in \[1\] (this paper refers to MCCV as repeated-learning testing-model).

------------------------------------------------------------------------

\[1\] Burman, P. (1989). A Comparative study of ordinary cross-validation, ð‘£v-fold cross validation and the repeated learing testing-model methods. *Bometrika* **76** 503-514.

\[2\] Hastie, T., Tibshirani, R. and Friedman, J. (2011). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second ed.* New York: Springer.

\[3\] Zhang, P. (1993). Model Selection Via Muiltfold Cross Validation. *Ann. Stat.* **21** 299â€“313

