---
title: "Monte Carlo Simulation Tools for REDD+ Uncertainty Estimates"
date: 2024-12-19
output: 
  pdf_document: 
    highlight: pygments
    toc: true
    toc_depth: 3
    latex_engine: xelatex

bibliography: references.bib
csl: american-chemical-society.csl
always_allow_html: true
df-print: kable      
editor_options: 
  markdown: 
    wrap: 120
---

```{r setup-1}
#| warning: false
#| message: false
#| error: false
#| include: false
#| echo: false
easypackages::packages(
  "animation", "allodb", "BIOMASS", "c2z", "caret", 
  "dataMaid", "DescTools","dplyr",
  "extrafont", "FawR", "flextable", "ForestToolsRS", 
  "formatR", "ggplot2", "htmltools",
  "janitor", "jsonlite", "lattice", "kableExtra", "kernlab",
  "knitr", "Mlmetrics", "olsrr", "plotly", "psych", "RColorBrewer",
  "rmarkdown", "readxl", "tibble", "tidymodels", "tidyverse",
  "tinytex", "truncnorm", "tune", "useful", "webshot", "webshot2", 
  prompt = F
  )
  
knitr::opts_chunk$set(
  echo    = TRUE, 
  message = FALSE, 
  warning = FALSE,
  error   = FALSE, 
  cache   = FALSE,
  comment = NA, 
  tidy.opts = list(width.cutoff = 60)
)

options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
```

```{css, echo=FALSE, class.source = 'foldable'}
div.column {
    display: inline-block;
    vertical-align: top;
    width: 50%;
}

#TOC::before {
  content: "";
  display: block;
  height: 80px;
  width: 210px;
  background-image: url(https://winrock.org/wp-content/uploads/2021/12/Winrock-logo-R.png);
  background-size: contain;
  background-position: center;
  background-position: 50% 50%;
  padding-top: 80px !important;
  background-repeat: no-repeat;
}
```

## 1. Introduction

When preparing for Monte Carlo simulations, it is best practice to start by examining descriptive statistics to
characterize the empirical distributions of input variables. This preliminary analysis typically includes statistical
tests for normality and visualizations of univariate distributions, such as histograms, kernel density plots, and Q-Q
plots. Together, these tools provide critical insights into the shape, spread, symmetry, skewness, and presence of
potential outliers in the data. Although this preliminary step may seem minor, it substantially influences uncertainty
estimates, which can directly translate into increased financial returns, particularly within forest project landscapes
exhibiting non-normal data distributions.

Accurately characterizing data distributions also helps in identifying and addressing biases, thereby ensuring high data
quality and increasing confidence in subsequent estimations of biomass and carbon emissions. Selecting appropriate
statistical distributions, informed by exploratory analyses, significantly enhances the reliability and precision of
Monte Carlo simulations. Consequently, such careful statistical characterizations reduce overall uncertainty in forest
biomass and emissions estimates. In turn, this strengthens the credibility of jurisdictional claims made under REDD+
programs and maximizes potential financial returns for Guyana from carbon financing initiatives.

Univariate distribution visualizations additionally provide auditors with useful diagnostic resources, enabling rapid
identification and characterization of biases commonly encountered in biomass data. These diagrams help auditors
efficiently assess the technical rigor and statistical approaches implemented by the project to monitor and manage
uncertainty (ART, 2021: 8). Winrock strongly recommends incorporating distribution analyses early in a project's
quantitative planning and throughout its technical standard operating procedures (SOPs). Such early integration
represents a low hanging fruit with cost-effective strategy and significant potential in reducing audit findings,
lowering uncertainty, and enhancing financial outcomes for Guyana's REDD+ activities. Specifically, early attention to
data distributions directly informs appropriate simulation selection from the available options in SimVoi.

To effectively guide practitioners and stakeholders in selecting appropriate statistical distributions for Monte Carlo
methods within forestry and REDD+ contexts, the following two tables present findings from a rapid review of relevant
literature. The review identified and summarized statistical distributions frequently encountered in forestry, biomass
estimation, and emissions analysis, which are disaggregated below between discrete and continuous types and according to
their inherent statistical characteristics.

###### *Table 1: Continuous data distributions, and example use cases for Monte Carlo simulations.* 

| Distribution | Statistical Criteria & Use Cases | PDF/PMF |
|------------------|-------------------------------------------------------------|-------------------------------------|
| **Normal (Gaussian)** | Symmetric, bell-shaped distribution used for modeling continuous variables such as biomass/ha. | $\displaystyle \begin{aligned}
f(x) &= \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{aligned}$ |
| **Lognormal** | Right-skewed distribution suitable for variables constrained to positive values (emission rates). | $\displaystyle f(x) = \frac{1}{x\sigma\sqrt{2\pi}} \exp\left(-\frac{(\ln x-\mu)^2}{2\sigma^2}\right)$ |
| **Exponential** | Models waiting times between independent events such as forest fire occurrences or logging events. | $\displaystyle \begin{aligned} f(x) &= \lambda e^{-\lambda x}\\[5pt] &,\; x\ge0 \end{aligned}$ |
| **Continuous Uniform** | Assumes all values in an interval [a, b] are equally likely, useful for random spatial sampling in forests. | $\displaystyle \begin{aligned} f(x) &= \frac{1}{b-a}\\[5pt] &,\; a\le x \le b \end{aligned}$ |
| **Chi-Square** | Often used in goodness-of-fit tests to evaluate model accuracy in biomass estimation. | $\displaystyle \begin{aligned} f(x) &= \frac{1}{2^{k/2}\Gamma(k/2)}\,x^{\frac{k}{2}-1}e^{-x/2}\\[5pt] &,\; x>0 \end{aligned}$ |
| **t-Distribution** | Suitable for small sample sizes and unknown variance, incl. forest carbon stocks of limited data. | $\displaystyle \begin{aligned}
f(x) &= \frac{\Gamma\left(\frac{v+1}{2}\right)}{\sqrt{v\pi}\,\Gamma\left(\frac{v}{2}\right)}\left(1+\frac{x^2}{v}\right)^{-\frac{v+1}{2}}
\end{aligned}$ |
| **Gamma** | Models positively skewed data, such as biomass growth rates or carbon accumulation over time. | $\displaystyle f(x) = \frac{x^{k-1}e^{-x/\theta}}{\theta^k\Gamma(k)}$ |
| **Weibull** | Flexible distribution used in reliability analysis, e.g., modeling tree mortality. | $\displaystyle f(x) = \frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}e^{-(x/\lambda)^k}$ |

###### *Table 2: Discrete data distributions, and example use cases designed with Monte Carlo simulations.*

| Distribution | Statistical Criteria & Use Cases | PMF |
|------------------|-------------------------------------------------------------|--------------------------------------|
| Bernoulli | Binary outcome probability, e.g., presence/absence of deforestation in a given area. | $\begin{aligned}P(X=x)&=p^{x}(1-p)^{1-x},\\ x&\in\{0,1\}\end{aligned}$ |
| Binomial | Probability of fixed number of successes over trials, e.g., count of deforestation events detected from satellite images. | $\begin{aligned}P(X=k)&=\binom{n}{k}p^{k}(1-p)^{n-k},\\ k&=0,1,\dots,n\end{aligned}$ |
| Poisson | Counts of independent events within intervals, e.g., number of wildfire incidents per year. | $\begin{aligned}P(X=k)&=\frac{\lambda^{k}e^{-\lambda}}{k!},\\ k&=0,1,2,\dots\end{aligned}$ |
| Geometric | Models number of trials until first success, e.g., number of inspections until identifying deforestation. | $\begin{aligned}P(X=k)&=(1-p)^{k-1}p,\\ k&=1,2,\dots\end{aligned}$ |
| Negative Binomial | Counts trials until r successes, useful for overdispersed data such as repeated deforestation detections. | $\begin{aligned}P(X=k)&=\binom{k+r-1}{k}(1-p)^{r}p^{k},\\ k&=0,1,2,\dots\end{aligned}$ |
| Discrete Uniform | Models equally likely discrete outcomes, e.g., random sampling of inventory plots across forest. | $\begin{aligned}P(X=x)&=\frac{1}{n},\\ x&=1,2,\dots,n\end{aligned}$ |

Discrete distributions describe forestry monitoring scenarios where data outcomes are countable and finite. Common
examples include the number of deforestation events, occurrences of wildfires, or counts of logged trees within a
defined monitoring interval. Accurate representation of discrete events using appropriate distributions such as
Binomial, Poisson, or Negative Binomial significantly enhances the accuracy of model predictions and uncertainty
assessments. For instance, employing a Poisson distribution to model occurrences of illegal logging events can improve
the precision of estimated deforestation emissions and reduce uncertainty around compliance risks.

In contrast, continuous distributions capture variables capable of taking any value within a specified range and are
particularly relevant in forestry when modeling measurements such as tree heights, carbon stock densities, or biomass
values. Continuous distributions like the Normal (Gaussian), Lognormal, Weibull, and Gamma distributions frequently
arise in ecological modeling and biomass estimations due to their ability to realistically represent ecological
variability and complex environmental factors. For example, using a Lognormal distribution for tree biomass data often
provides more reliable estimates, particularly when the dataset is right-skewed due to natural variability in tree
growth and forest conditions.

Central to these distributions are two mathematical concepts: Probability Mass Functions (PMFs) for discrete data and
Probability Density Functions (PDFs) for continuous data. PMFs allocate specific probabilities to discrete outcomes,
essential for accurately simulating events such as species occurrences or forest disturbances. PDFs describe the
relative likelihood of continuous data points, enabling the robust estimation of variables like forest carbon content or
annual biomass increment.

In Monte Carlo simulations, precise definition and utilization of PMFs and PDFs are crucial. These functions underpin
random sampling processes that directly influence the reliability, precision, and credibility of uncertainty estimates.
Given that forestry data is known to exhibit non-normal distributions due to inherent ecological heterogeneity that,
informed selection and rigorous application of these functions are vital. Accurate modeling of the underlying data
distribution enhances biomass and emissions estimates, significantly reduces uncertainty, and bolsters the financial and
ecological credibility of REDD+ reporting initiatives (Morgan & Henrion, 1990; IPCC, 2019; ART, 2021).

Practitioners are encouraged to conduct exploratory data analysis early in their project planning stages, integrating
statistical tests of normality and visual assessments (histograms, kernel density plots, Q-Q plots). Such preliminary
analyses assist in diagnosing data distributions accurately, improving model selection, reducing potential auditor
findings, and ultimately enhancing the financial and environmental outcomes of national REDD+ monitoring programs.

## 2. Method

### Import data

```{r, class.source = c("numCode", "r", "numberLines"), fig.show='hold', out.height="50%"}
# Point this to the correct path where your file is located:
workbook  = "./data/art/GuyanaARTWorkbookMC-thru2022-April2024_values.xlsx"
CarbonStocks = readxl::read_excel(workbook, "CarbonStocks") |> 
  janitor::clean_names() |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))
CarbonStocks_MC = readxl::read_excel(workbook, "CarbonStocks (MC)") |> 
  janitor::clean_names() |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))

DeforestationEF = readxl::read_excel(workbook, "Deforestation EFs") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))
DeforestationEF_MC = readxl::read_excel(workbook, "Deforestation EFs (MC)") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))

DegradationEF = readxl::read_excel(workbook, "Degradation EFs") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))
DegradationEF_MC = readxl::read_excel(workbook, "Degradation EFs (MC)") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))

ActivityData = readxl::read_excel(workbook, "Activity Data") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))
ActivityData_MC = readxl::read_excel(workbook, "Activity Data (MC)") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))

Emissions = readxl::read_excel(workbook, "CarbonStocks") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))
Emissions_MC = readxl::read_excel(workbook, "CarbonStocks (MC)") |> 
  mutate(across(where(is.numeric), ~ round(.x, 1)))

# Vislualize
flextable(head(CarbonStocks[, 1:8])) |> fontsize(size = 8, part = "all")
flextable(head(CarbonStocks_MC[, 1:8])) |> fontsize(size = 8, part = "all")
flextable(head(CarbonStocks[, 1:8])) |> fontsize(size = 8, part = "all")
flextable(head(CarbonStocks_MC[, 1:8])) |> fontsize(size = 8, part = "all")
# (Optionally un-comment these to view other data frames)
# flextable(head(DeforestationEF_MC[, 1:8])) |> fontsize(size = 8, part = "all")
# flextable(head(DeforestationEF[, 1:8])) |> fontsize(size = 8, part = "all")
# flextable(head(ActivityData[, 1:8])) |> fontsize(size = 8, part = "all")
# flextable(head(ActivityData_MC[, 1:8])) |> fontsize(size = 8, part = "all")
# flextable(head(Emissions[, 1:8])) |> fontsize(size = 8, part = "all")
# flextable(head(Emissions_MC[, 1:8])) |> fontsize(size = 8, part = "all")
# flextable(head(DegradationEF[, 1:8])) |> fontsize(size = 8, part = "all")
# flextable(head(DegradationEF_MC[, 1:8])) |> fontsize(size = 8, part = "all")
#dplyr::glimpse(CarbonStocks)
```

### Tidy data

Achieving tidy data is crucial for robust analysis, particularly when working with datasets imported from Excel, which
often require adjustments to column names, strata labels, data types, and row layouts, especially when summary
statistics are presented in non-standard formats. Begin by identifying the relevant rows and columns for each pool,
specifically those containing mean, standard deviation, minimum, maximum, and confidence interval values. The `dplyr`
package provides a powerful and efficient means for manipulating dataframes, facilitating these necessary adjustments.

Assuming the rows in the "CarbonStocks_MC" sheet maintain a consistent order, operations can be performed by simply
referencing the dataframe name. A common approach involves reshaping the data so that each row represents a "Statistic,"
such as mean or standard deviation, and each column corresponds to a carbon pool, like "AG Tree" or "BG Tree."

Initially, select the columns pertinent to your carbon pools, for instance, those named "AG Tree (tC/ha)" or "BG Tree
(tC/ha)," and rename them to align with the "SimVoi" workbook. Subsequently, extract the rows containing the summary
statistics, typically the first few rows, and proceed to reshape the data. Note that direct renaming of row values, such
as assigning "mean" to the second row, must be explicitly performed if required, as it is not automatically handled.

Throughout this process, thorough inspection of the data using `view(CarbonStocks)` or `glimpse(CarbonStocks)` after
each operation is strongly recommended. This ensures accurate mapping of rows and columns to the "Statistic" labels.

To effectively transpose the data and transition between wide and long formats, utilize the `tidyr` package's
`pivot_longer()` and `pivot_wider()` functions. These functions facilitate the transformation of data such that each row
contains a data value, such as mean, standard deviation, or minimum, and each column represents a variable or carbon
pool. Be mindful of potential missing or incorrect column names, often indicated by placeholder names like "...1"
generated by `readxl::read_excel()`; these must be renamed. Finally, pivot the data from long format back to wide,
ensuring that "Statistic" becomes a distinct column and the carbon pools, such as "AG_Tree" and "BG_Tree," are
represented as separate variable columns.

```{r, class.source = c("numCode", "r", "numberLines"), fig.show='hold', out.height="50%"}
#CarbonStocks <- CarbonStocks |> rename(Statistic = `...1`)

CarbonStocks <- CarbonStocks %>%
  select(
    `Statistic`           = 1,
    `AG_Tree`             = 2,
    `BG_Tree`             = 3,
    `Saplings`            = 4,
    `StandingDeadWood`    = 5,
    `LyingDeadWood`       = 6,
    `SumCarbonNoLitter`   = 7,
    `Litter`              = 8,
    `SumCpoolWLitter`     = 9,
    `SumCO2e`             = 10,
    `Soil_tC_ha`          = 11,
    `SumALL_POOLS_CO2eha` = 12,
    `SumABGBLiveTree`     = 13
  ) %>% slice(1:9)
# Rename missing column names

# Convert from wide to long: "Statistic" will define the row identity
CarbonStocks_long <- CarbonStocks |>
  tidyr::pivot_longer(
    cols = -Statistic,
    names_to = "Pool",
    values_to = "Value"
  ) |>
  mutate(Value = as.numeric(Value))

# Convert from long back to wide format:
CarbonStocks_wide <- CarbonStocks_long %>%
  pivot_wider(
    names_from = Statistic,
    values_from = Value
  )

# Inspect the final structure
CarbonStocks_wide

# Example summarizing a particular column:
CarbonStocks_wide %>%
  summarise(
    Mean_AGTree = mean(`AG Tree (tC/ha)`, na.rm=TRUE),
    SD_AGTree   = sd(`AG Tree (tC/ha)`, na.rm=TRUE)
  )


# Transpose to long dataframe: flipping rows w/ columns
CarbonStocks_long <- CarbonStocks |>
  tidyr::pivot_longer(
    cols = -Statistic,
    names_to = "Pool",
    values_to = "Value"
  ) |> mutate(Value = as.numeric(Value))

# Pivot back to wide dataframe & “Statistic” becomes a row:
CarbonStocks_wide <- CarbonStocks_long %>%
  pivot_wider(
    names_from = Statistic,
    values_from = Value
  )
# Inpspect
CarbonStocks_wide

CarbonStocks_wide |>
  summarise(Mean_AGTree = mean(`AG Tree (tC/ha)`, na.rm=TRUE),
    SD_AGTree   = sd(`AG Tree (tC/ha)`, na.rm=TRUE))
```

### Descriptive Statistics

# Shapiro-Wilk test for normality

shapiro.test(CarbonStocks\$`AG Tree (tC/ha)`)

### Distribution Analysis

# Example approximate histogram

hist(AG_draws, breaks=30, main="AG Tree (approx. distribution)") qqnorm(AG_draws) qqline(AG_draws, col="red")

The Coefficient of Variation `CV` is a standardized, unit-less measure of dispersion defined as the ratio of the
standard deviation to the mean, typically expressed as a percentage. This standardization allows for meaningful
comparisons of variability across datasets or scales, regardless of the underlying units, offering helpful tool for
assessing novel data from periodic field inventories or mapping updates.

$$
\mathrm{CV} = \frac{\sigma}{\mu} \times 100\%
$$

$$
\mathrm{CV}_{\%} = 100 \times \frac{\text{std. dev}}{\text{mean of all plots (calculated)}}
$$

For these carbon stocks, a higher CV indicates greater relative variability or "scatter" in the data. While the CV is a
useful indicator of dispersion and can signal potential non-normality, it does not provide any information on the
direction of skew in the distribution.

In our analysis, the CV was computed below within a helper function called `calc_derived_stats`. This function not only
calculates the CV but also compares the reported 90% confidence interval with the standard deviation, which, under
normality, should approximate to ±1.645 × SD. This iterative scoring helps assess the internal consistency of the
reported descriptive statistics.

```{r, class.source = c("numCode", "r", "numberLines"), fig.show='hold', out.height="50%", eval=F}
# Helper function of derived descriptive statistics:
calc_derived_stats <- function(df) {
  df %>%
    mutate(
      CV_percent = 100 * (`std. dev` / `mean of all plots (calculated)`),
      sd_implied_by_90CI = `90% CI` / 1.645,
      SDs_below_mean = (`mean of all plots (calculated)` - minimum) / `std. dev`,
      SDs_above_mean = (maximum - `mean of all plots (calculated)`) / `std. dev`
    )
  }

CarbonStocks_stats <- calc_derived_stats(CarbonStocks_wide)
#CarbonStocks_stats # Remember to inspect new variables 

# Custom function to simulate from each row (assuming truncnormal)
simulate_truncnorm_from_summary <- function(
  mean_val, sd_val, min_val=0, max_val=Inf, 
  # We loaded here the 'truncnorm' package from main cran libraries, I will add to in-line 
  n_draws=10000) {
  draws <- truncnorm::rtruncnorm(
    n     = n_draws,
    a     = min_val,
    b     = max_val,
    mean  = mean_val,
    sd    = sd_val
  )
  # Return vector of draws
  return(draws)
}

# Repeat for AG_Tree
ag_tree_stats <- CarbonStocks_stats %>% filter(Pool == "AG_Tree")
AG_mean <- ag_tree_stats$`mean of all plots (calculated)`
AG_sd   <- ag_tree_stats$`std. dev`
AG_min  <- ag_tree_stats$minimum
AG_max  <- ag_tree_stats$maximum

# We may vote to do a = 0 if we never allow negative carbon:
AG_draws <- simulate_truncnorm_from_summary(
  mean_val = AG_mean, 
  sd_val   = AG_sd, 
  min_val  = 0,     # or AG_min if you prefer
  max_val  = Inf, 
  n_draws  = 10000
)

# Compare results:
mean(AG_draws)
sd(AG_draws)
min(AG_draws)
max(AG_draws)
quantile(AG_draws, probs = c(0.05, 0.95))


# Quick histogram of the draws
hist(AG_draws, breaks=40, col="skyblue", 
     main="Truncated Normal draws for AG Tree",
     xlab="AG Tree (tC/ha)")

# If you want to do this for each carbon pool in a loop, 
# you can add a small function:

simulate_all_pools <- function(df, n_draws=10000) {
  # df is your cs_stats data frame
  # Return a named list of random draws
  out <- list()
  for (i in seq_len(nrow(df))) {
    rowi <- df[i, ]
    pool_name <- rowi$Pool
    mean_val  <- rowi$`mean of all plots (calculated)`
    sd_val    <- rowi$`std. dev`
    # Use zero for min bound; or rowi$minimum if you want to
    # replicate the workbook min
    draws <- rtruncnorm(
      n=n_draws,
      a=0, 
      b=Inf,
      mean=mean_val,
      sd=sd_val
    )
    out[[pool_name]] <- draws
  }
  return(out)
}

all_draws <- simulate_all_pools(CarbonStocks_st_stats, n_draws=10000)


names(l_draws)




ggplot(data.frame(AG_draws), aes(x = AG_draws)) +
  geom_histogram(aes(y = ..density..), bins = 50, fill = "skyblue", alpha = 0.7) +
  geom_density(col = "red") +
  labs(title = "Monte Carlo Simulation of AG Tree Carbon Pool",
       x = "Carbon Stock (tC/ha)", y = "Density")

```

## Replicating SimVoi

We utilize the replicate function to repeat a simulationfollowing a randomized normally truncated multiple times with
`replicate(n=10000`, while determining the size of the sampled subset with `rnorm(n=100`. The first model explores
sample size parameters only, replication parameters are tested below this in comparisons.

```{r, class.source = c("numCode", "r", "numberLines"), fig.show='hold', out.height="50%", eval=F}
MEAN = CarbonStocks$`AG Tree (tC/ha)`[1]
SD   = CarbonStocks$`AG Tree (tC/ha)`[2]

randtruncnormal_sim_10000 <- rnorm(n=10000,mean=MEAN,sd=SD)
hist(randtruncnormal_sim_10000, freq=F)
AG_Tree_tC_ha   = mean(randtruncnormal_sim_10000)
AG_Tree_tCO2_ha = AG_Tree_tC_ha*(44/12)
AG_Tree_tC_ha
AG_Tree_tCO2_ha
#curve(dnorm(x, mean=MEAN, sd=SD), from=0, to=450, add=T, col="red")
```

## Compare simulations

```{r, class.source = c("numCode", "r", "numberLines"), fig.show='hold', out.height="50%", eval=F}
# 10,000 simulations sampling 10 observations
randtruncnormal_sim_10000_10 = replicate(
  n=10000, rnorm(n=10, mean=MEAN,sd=SD))
hist(apply(X = randtruncnormal_sim_10000_10, MARGIN=2, FUN=mean))
sd(apply(X = randtruncnormal_sim_10000_10, MARGIN=2, FUN=mean))
mean(apply(X = randtruncnormal_sim_10000_10, MARGIN=2, FUN=mean))
(mean(apply(X = randtruncnormal_sim_10000_10, MARGIN=2, FUN=mean)))*(44/12)

# 10,000 simulations sampling 100 observations
randtruncnormal_sim_10000_100 = replicate(
  n=10000,rnorm(n=100, mean=MEAN,sd=SD))
hist(apply(X = randtruncnormal_sim_10000_100, MARGIN=2, FUN=mean))
sd(apply(X = randtruncnormal_sim_10000_100, MARGIN=2, FUN=mean))
mean(apply(X = randtruncnormal_sim_10000_100, MARGIN=2, FUN=mean))
(mean(apply(X = randtruncnormal_sim_10000_100, MARGIN=2, FUN=mean)))*(44/12)

# 10,000 simulations sampling 1,000 observations
randtruncnormal_sim_10000_1000 = replicate(
  n=10000, rnorm(n=1000, mean=MEAN, sd=SD))
hist(apply(X = randtruncnormal_sim_10000_1000, MARGIN=2, FUN=mean))
sd(apply(X = randtruncnormal_sim_10000_1000, MARGIN=2, FUN=mean))
mean(apply(X = randtruncnormal_sim_10000_1000, MARGIN=2, FUN=mean))
(mean(apply(X = randtruncnormal_sim_10000_1000, MARGIN=2, FUN=mean)))*(44/12)

# 10,000 simulations sampling 10,000 observations
randtruncnormal_sim_10000_10000 = replicate(
  n=10000, rnorm(n=10000, mean=MEAN, sd=SD))
hist(apply(X = randtruncnormal_sim_10000_10000, MARGIN=2, FUN=mean))
sd(apply(X = randtruncnormal_sim_10000_10000, MARGIN=2, FUN=mean))
mean(apply(X = randtruncnormal_sim_10000_10000, MARGIN=2, FUN=mean))
(mean(apply(X = randtruncnormal_sim_10000_10000, MARGIN=2, FUN=mean)))*(44/12)

```

```{r, class.source = c("numCode", "r", "numberLines"), fig.show='hold', out.height="50%", eval=F}
#| eval: true
devtools::session_info()
#Sys.getenv()
#.libPaths()
```

## References

## Annex I: SimVoi Functions & Syntax

SimVoi adds seventeen random number generator functions defined with the following syntax:

-   `RandBeta(alpha,beta,,[MinValue],[MaxValue])`
-   `RandBinomial(trials,probability_s)`
-   `RandBiVarNormal(mean1,stdev1,mean2,stdev2,correl12)`
-   `RandCumulative(value_cumulative_table)`
-   `RandDiscrete(value_discrete_table)`
-   `RandExponential(lambda)`
-   `RandInteger(bottom,top)`
-   `RandLogNormal(Mean,StDev)`
-   `RandNormal(mean,standard_dev)`
-   `RandPoisson(mean)`
-   `RandSample(population)`
-   `RandTriangular(minimum,most_likely,maximum)`
-   `RandTriBeta(minimum,most_likely,maximum,[shape])`
-   `RandTruncBiVarNormal(mean1,stdev1,mean2,stdev2,correl12, [min1],[max1],[min2],[max2])`
-   `RandTruncLogNormal(Mean,StDev,[MinValue],[MaxValue])`
-   `RandTruncNormal(Mean,StDev,[MinValue],[MaxValue])`
-   `RandUniform(minimum,maximum)`

In the following, we attempt to match the SimVoi Excel formula of

`=[1]!randtruncnormal(CarbonStocks.B2,CarbonStocks.B3,0)`

function, as closely as random seeding allows. According to package documentation, the `RandTruncNormal()` function
"*Returns a random value from a truncated normal probability density function. This function can model an uncertain
quantity with a bell-shaped density function where extreme values in the tails of the distribution are not desired."*

In terms of simulation parameters, *"RandTruncNormal(Mean,StDev,MinValue,MaxValue)) uses values of RandNormal until a
value is found between MinValue and MaxValue or until it has made 10,000 attempts."* The above formula provides a
minimum value of `0`, passing to the default number of simulations of 10,000.

Annex II: Rapid literature review or Monte Carlo methods in REDD+

###### Table A.2: Search parameters, resource scope, and objectives informing search

-   ART (2021). *TREES: The REDD+ Environmental Excellence Standard*. Architecture for REDD+ Transactions.

-   Bolker, B. (2008). *Ecological Models and Data in R.* Princeton University Press.

-   Hilbe, J. M. (2014). *Modeling Count Data.* Cambridge University Press.

-   Limpert, E., Stahel, W. A., & Abbt, M. (2001). "Log-normal distributions across the sciences: Keys and clues."
    *BioScience*, 51(5), 341–352.

-   Morgan, M. G., & Henrion, M. (1990). *Uncertainty: A Guide to Dealing with Uncertainty in Quantitative Risk and
    Policy Analysis.* Cambridge University Press.

-   Ross, S. M. (2019). *Introduction to Probability Models* (12th ed.). Academic Press.
